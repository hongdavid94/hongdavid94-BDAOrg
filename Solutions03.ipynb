{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03 solutions\n",
    "\n",
    "**Due:** Thursday, 2018-03-01, 11:59 PM, as a Jupyter notebook submitted via your repo in the course GitHub organization.  Edit the provided Solutions03 notebook with your solutions.  All  subproblems (or problems with no subproblems) are worth 1 point unless otherwise noted.\n",
    "\n",
    "In derivations, use a bit of text to explain your steps, but you needn't write an essay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (2 points):\n",
    "\n",
    "> Use the \"or\" and \"and\" rules to find an expression for $P(A\\lor B \\lor C)$, *not* assuming that $A$, $B$, and $C$ are mutually exclusive. Seek a result using only probabilities for the individual propositions, and \"and\" combinations of them. (You can use a comma instead of $\\land$ for \"and\" if you wish.)  You may use associativity of \"or\" and \"and;\" that is, $A\\lor (B \\lor C) \\equiv (A\\lor B) \\lor C$, and $A\\land (B \\land C) \\equiv (A\\land B) \\land C$.\n",
    "\n",
    "> Do this in two steps:\n",
    "\n",
    "> * Copy the following incomplete truth tables into your solution cell and complete them. For each table, you should find that the two bold columns have the same truth values, indicating they are logically equivalent. These tables thus establish two *rules of replacement*: where you see one of these formulas, you may substitute the other.  MathJax will probably make the headings hard to read, so to clarify: the first table aims to show $X\\land Y\\land X \\equiv X\\land Y$ (so you can drop repeated symbols in a multiple \"and\"), and the second table aims to show a kind of distributive rule: $X\\land (Y\\lor Z) \\equiv (X\\land Y)\\lor (X\\land Z)$.\n",
    "\n",
    "> *Hint*: You may find it easier to work on the tables using the [Markdown Tables generator - TablesGenerator.com](https://www.tablesgenerator.com/markdown_tables) web page. Note that you can copy the Markdown for a table to your clipboard, and load it into the web page using the `File->Paste` command on the web page.\n",
    "\n",
    "> * With those replacement rules in hand, proceed with deriving the 3-proposition \"or\" rule.\n",
    "\n",
    "> *Hint*: You might guess from the 2-proposition \"or\" rule that the answer is $P(A\\lor B \\lor C) = P(A) + P(B) + P(C) - P(A\\land B \\land C)$. But that's not right. It's quite a bit more complicated (which is why having mutually exclusive alternatives is a great help)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work:\n",
    "\n",
    "| $X$ | $Y$ | ${\\bf X\\land Y}$ | ${\\bf (X\\land Y) \\land X}$ |\n",
    "|---|-----|------------|----------------------|\n",
    "| 0   | 0   |      0      | 0                 |\n",
    "| 0   | 1   |      0      | 0                    |\n",
    "| 1   | 0   |      0      | 0                     |\n",
    "| 1   | 1   |      1      | 1                      |\n",
    "\n",
    "| $X$ | $Y$ | $Z$ | $Y\\lor Z$ | ${\\bf X\\land (Y\\lor Z)}$ | $X\\land Y$ | $X\\land Z$ | ${\\bf (X\\land Y)\\lor (X\\land Z)}$ |\n",
    "|-----|-----|-----|-----------|--------------------------|------------|------------|-----------------------------------|\n",
    "| 0   | 0   | 0   | 0         | 0                        | 0          | 0          | 0                                 |\n",
    "| 0   | 0   | 1   | 1         | 0                        | 0          | 0          | 0                                 |\n",
    "| 0   | 1   | 0   | 1         | 0                        | 0          | 0          | 0                                 |\n",
    "| 0   | 1   | 1   | 1         | 0                        | 0          | 0          | 0                                 |\n",
    "| 1   | 0   | 0   | 0         | 0                        | 0          | 0          | 0                                 |\n",
    "| 1   | 0   | 1   | 1         | 1                        | 0          | 1          | 1                                 |\n",
    "| 1   | 1   | 0   | 1         | 1                        | 1          | 0          | 1                                 |\n",
    "| 1   | 1   | 1   | 1         | 1                        | 1          | 1          | 1                                 |\n",
    "\n",
    "| $X$ | $Y$ | $Z$ | ${\\bf X\\land Y \\land Z}$ | $X\\land Z$ | $X\\land Y$ | ${\\bf(X\\land Y)\\land(X\\land Z)}$ |\n",
    "|-----|-----|-----|--------------------|------------|------------|---------------------------------|\n",
    "| 0   | 0   | 0   | 0                  | 0          | 0          | 0                               |\n",
    "| 0   | 0   | 1   | 0                  | 0          | 0          | 0                               |\n",
    "| 0   | 1   | 0   | 0                  | 0          | 0          | 0                               |\n",
    "| 0   | 1   | 1   | 0                  | 0          | 0          | 0                               |\n",
    "| 1   | 0   | 0   | 0                  | 0          | 0          | 0                               |\n",
    "| 1   | 0   | 1   | 0                  | 1          | 0          | 0                               |\n",
    "| 1   | 1   | 0   | 1                  | 0          | 1          | 0                               |\n",
    "| 1   | 1   | 1   | 1                  | 1          | 1          | 1                               |\n",
    "\n",
    "From the truth tables above we can see that $(X\\land Y) \\land (X\\land Z)$ and $(X \\land Y \\land Z)$ are logically equivalent.\n",
    "We also learn that $X\\land (Y\\lor Z)$ and $(X\\land Y) \\lor (X\\land Z)$ are logically equivalent.\n",
    "\n",
    "$P(A\\lor B \\lor C) = P((A\\lor B) \\lor C)$\n",
    "\n",
    "$= P(A\\lor B) + P(C) - P((A\\lor B) \\land C)$\n",
    "\n",
    "$= P(A)+P(B)+P(C) -P(A\\land B) - P((A\\land C)\\lor(B\\land C))\\; (\\because \\; X\\land (Y\\lor Z) \\equiv (X\\land Y) \\lor (X\\land Z))$\n",
    "\n",
    "$= P(A)+P(B)+P(C) -P(A\\land B) - P(A\\land C) - P(B\\land C) + P((A\\land C) \\land (B\\land C)) $\n",
    "\n",
    "$= P(A)+P(B)+P(C) -P(A\\land B) - P(A\\land C) - P(B\\land C) + P(A\\land B \\land C)\\; (\\because \\; (X\\land Y) \\land (X\\land Z) \\equiv (X \\land Y \\land Z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: $P(A\\lor B \\lor C)= P(A)+P(B)+P(C) -P(A\\land B) - P(A\\land C) - P(B\\land C) + P(A\\land B \\land C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "\n",
    "> Prove the consistency of chained and joint inferences based on using two datasets to estimate a parameter, $\\theta$.\n",
    "\n",
    "> 1. Use Bayes's theorem to write down the posterior PDF for $\\theta$ based on data $D_1$.\n",
    "> 2. Use the posterior from (1) as the prior for inference of $\\theta$ additionally considering new data, $D_2$, using Bayes's theorem to compute an overall posterior PDF for $\\theta$, $p(\\theta|D_1,D_2,\\mathcal{C})$. *Do not assume that the joint sampling distribution for $(D_1,D_2)$ factors*:\n",
    "$$\n",
    "p(D_1,D_2|\\theta) \\ne p(D_1|\\theta)\\times p(D_2|\\theta). \\qquad ||\\; \\mathcal{C}\n",
    "$$\n",
    "> 3. Now suppose you start with the same initial prior used in (1), but consider the two datasets together. Compute the posterior $p(\\theta|D_1,D_2,\\mathcal{C})$ in a single step, considering $(D_1,D_2)$ as a single, pooled dataset.\n",
    "> 4. Show that the results of (2) and (3) are equal.\n",
    "\n",
    "> For convenience, you may drop $\\mathcal{C}$ from the notation, since the same contextual information is being used throughout.  *Hint:* You shouldn't have to write out any marginal likelihoods (e.g., in terms of integrals of prior times likelihood) in order to prove consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work\n",
    "\n",
    "\n",
    "$\\large 1. \\hspace{10mm} P(\\theta | D_1) = \\frac{P(D_1 | \\theta) \\times P(\\theta)}{P(D_1)}$\n",
    "\n",
    "$\\large 2. \\hspace{10mm} P(\\theta | D_2) = \\frac{P(D_2 | \\theta) \\times P(\\theta)}{P(D_2)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta) = \\frac{P(\\theta | D_2) \\times P(D_2)}{P(D_2 | \\theta)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1) = \\frac{P(D_1 | \\theta) \\times P(\\theta | D_2) \\times P(D_2)}{P(D_1) \\times P(D_2 | \\theta)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1) = \\frac{P(D_1 | \\theta) \\times P(D_2 | \\theta) \\times P(\\theta | D_2) \\times P(D_2)}{P(D_1) \\times P(D_2 | \\theta)^2}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1) = \\frac{P(D_1 | \\theta) \\times P(D_2 | D_1, \\theta) \\times P(\\theta | D_2) \\times P(D_2)}{P(D_1) \\times P(D_2 | \\theta)^2}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1) = \\frac{P(D_1, D_2 | \\theta) \\times P(\\theta | D_2) \\times P(D_2)}{P(D_1) \\times P(D_2 | \\theta)^2}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1) = \\frac{P(\\theta | D_1, D_2) \\times P(D_1,D_2) \\times P(\\theta | D_2) \\times P(D_2)}{P(D_1) \\times P(D_2 | \\theta)^2 \\times P(\\theta)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = \\frac{P(\\theta | D_1) \\times P(D_1) \\times P(D_2 | \\theta)^2 \\times P(\\theta)}{P(D_1,D_2) \\times P(\\theta | D_2) \\times P(D_2)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = \\frac{P(\\theta | D_1) \\times P(D_1) \\times P(D_2 | \\theta)^2}{P(\\theta | D_2) \\times P(D_2)} \\times \\frac{P(\\theta)}{P(D_1,D_2)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = \\frac{P(\\theta, D_1) \\times P(D_1) \\times P(D_2, \\theta)^2 \\times P(D_2)}{P(\\theta ,D_2) \\times P(D_2) \\times P(D_1) \\times P(\\theta) \\times P(\\theta)} \\times \\frac{P(\\theta)}{P(D_1,D_2)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = \\frac{P(\\theta, D_1) \\times P(D_2, \\theta)}{P(\\theta) \\times P(\\theta)} \\times \\frac{P(\\theta)}{P(D_1,D_2)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = P(D_1 | \\theta) \\times P(D_2| \\theta)\\times \\frac{P(\\theta)}{P(D_1,D_2)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = P(D_1 | \\theta) \\times P(D_2| D_1, \\theta)\\times \\frac{P(\\theta)}{P(D_1,D_2)}$\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = P(D_1, D_2| \\theta)\\times \\frac{P(\\theta)}{P(D_1,D_2)} - 2$\n",
    "\n",
    "$\\large 3. \\hspace{10mm} Let \\> D_t = \\{D_1,D_2\\} $\n",
    "\n",
    "$\\large \\hspace{14mm} P(\\theta | D_1, D_2) = P(\\theta | D_t) = P(D_t | \\theta) \\times \\frac{P(\\theta)}{P(D_t)} = P(D_1,D_2 | \\theta) \\times \\frac{P(\\theta)}{P(D_1,D_2)} - 3$\n",
    "\n",
    "$\\large 4. \\hspace{10mm} \\therefore$ \n",
    "## 2 and 3 are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (3 points):\n",
    "\n",
    "> *Suppose you assign a symmetric Dirichlet PDF to a problem with $K=4$ categories. What are the implications of such a prior if you **aggregate** pairs of categories?*\n",
    "> * Start with the symmetric Dirichlet PDF for $K=4$, with concentration parameter $\\kappa$:\n",
    "$$\n",
    "p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\;\\propto\\;\n",
    "  \\alpha_1^{\\kappa-1}\\alpha_2^{\\kappa-1}\\alpha_3^{\\kappa-1}\\alpha_4^{\\kappa-1}\\;\n",
    "  \\delta\\left(1-\\sum_k\\alpha_k\\right).\n",
    "$$\n",
    "Note that here we've dropped the gamma functions comprising the normalization constant; you can (and should!) ignore parameter-independent constants throughout this problem (use the MathJax `\\propto` symbol to indicate proportionality).\n",
    "> * Consider the derived parameter $\\beta \\equiv \\alpha_1 + \\alpha_2$, the probability for an outcome being in either category 1 or category 2.  (Note that $1-\\beta = \\alpha_3 + \\alpha_4$, corresponding to aggregating the remaining two categories).  Since $\\beta$ is a function of the $\\alpha$'s, the prior for the $\\alpha$'s implies a prior for $\\beta$.  The prior PDF for $\\beta$ can be found using LTP as follows:\n",
    "\\begin{align}\n",
    "p(\\beta)\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\beta, \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\qquad \\mbox{[extend the conversation]}\\\\\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\; \n",
    "     p(\\beta | \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\qquad \\mbox{[product rule]}\\\\\n",
    "  &= \\int d\\alpha_1 \\cdots \\int d\\alpha_4\\; p(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\; \n",
    "     \\delta(\\beta - [\\alpha_1+\\alpha_2]),\n",
    "\\end{align}\n",
    "where in the last line we used\n",
    "$$\n",
    "p(\\beta | \\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) = \\delta(\\beta - [\\alpha_1+\\alpha_2]).\n",
    "$$\n",
    "That is, if we know the $\\alpha$'s, then we know that $\\beta$ must be exactly equal to $(\\alpha_1 + \\alpha_2)$, which we can enforce with a $\\delta$ function (by construction, a PDF concentrated at a point).\n",
    "\n",
    "> Your task is to do the integral giving $p(\\beta)$ and explore the result, as follows (do each step separately).\n",
    "\n",
    "> 1. Plug the expression for the $\\alpha$ prior into the equation for $p(\\beta)$; the integrand will have *two* $\\delta$ functions in it, one from the $\\alpha$ prior, and one fixing $\\beta$. \n",
    "> 2. Note that the $\\beta$-dependent $\\delta$ function doesn't depend on $(\\alpha_3,\\alpha_4)$. Isolate the parts depending on $(\\alpha_3,\\alpha_4)$, and do the double integral over $\\alpha_3$ and $\\alpha_4$ using the GBI.\n",
    "> 3. Integrals over $\\alpha_1$ and $\\alpha_2$ remain.  Use the remaining $\\delta$ function to do the $\\alpha_2$ integral.\n",
    "> 4. Finally, do the $\\alpha_1$ integral using the GBI, leaving only an expression in terms of $\\beta$ and constants.\n",
    "> 5. You should find that the prior for $\\beta$ is a beta distribution (i.e., a $K=2$ Dirichlet).  What value of $\\kappa$ would make this beta distribution equal to the uniform distribution we used as a prior for inference with binomial data?  Is the original 4-category $\\alpha$ prior corresponding to this $\\kappa$ uniform with respect to $(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1:\n",
    "\n",
    "> Calculate and plot the posterior PDF using the analytic formula from lecture:\n",
    "> * Use the scipy.stats.norm distribution object to generate a single sample of $N$ observations, $d_i$, following the model in the lecture.  Pick your own $N$, and your own \"true\" values of the parameters $\\mu$ and $\\sigma$ for the observations.\n",
    "> * Pick a prior mean, $\\mu_0$, and standard deviation, $w_0$, defining a normal prior.  Plot the posterior PDF for $\\mu$ using the formula presented in class for the conjugate posterior (the formula with the quantity $B$ specifying how much the posterior shrinks toward the prior).  Use the numpy `linspace` function to make an array of $\\mu$ values over which you'll evaluate the PDF.  You may use either the scipy.stats `norm` object, or explicit calculation (with `exp`, etc.), to evaluate the PDF.  Use a thick solid curve for the plot (say, with lw=2 or 3 in the matplotlib `plot` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2:\n",
    "\n",
    "> Now explicitly calculate and plot the posterior PDF from the prior and likelihood: \n",
    "* Use the same grid of $\\mu$ values used for Problem 4.1.\n",
    "* Evaluate the normal prior and the likelihood function on the grid.\n",
    "* Calculate the prior $\\times$ likelihood, and normalize it using the trapezoid rule (code the trapezoid rule explicitly; don't use `numpy.trapz`).\n",
    "* Plot the resulting normalized PDF on the same axes as Problem 4.1.  Use a dashed line style (and optionally transparency, via the `alpha` argument to `plot`) so that both curves are visible (they should overlap!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3:\n",
    "\n",
    "> Create test cases that verify elements of your computation:\n",
    "* Create a case that checks whether your trapezoid rule integration matches the result given by `numpy.trapz`.\n",
    "* Create a case that checks whether the two posterior PDFs match over the grid of $\\mu$ values.\n",
    "* Include a `nosetests` run in your notebook that verifies the tests pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
